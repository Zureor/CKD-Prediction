# -*- coding: utf-8 -*-
"""Particle Swarm Optimization-Based Minimal Feature Selection for Interpretable CKD Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wtdk6cX03NTppe37SN_qQ4fiDlepOwIA

# **Importing Basics**
"""

import pandas as pd
import numpy as np

"""# **Loading and Preparing Data**"""

df = pd.read_csv('/content/kidney_disease.csv')

# dropping id column
df.drop('id', axis = 1, inplace = True)

# rename column names to make it more user-friendly

df.columns = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',
              'pus_cell_clumps', 'bacteria', 'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',
              'potassium', 'haemoglobin', 'packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count',
              'hypertension', 'diabetes_mellitus', 'coronary_artery_disease', 'appetite', 'peda_edema',
              'aanemia', 'class']
df.head(400)

"""# **Data Preprocessing**"""

df.info()

# 2. Clean categorical columns (keep NaNs)

df['class'] = df['class'].astype(str).str.strip().str.lower()

cat_cols = [col for col in df.columns if df[col].dtype == 'object' and col != 'class']

for col in cat_cols:
    df[col] = df[col].apply(lambda x: x.strip().lower() if isinstance(x, str) else x)
    df[col] = df[col].astype('object')
    # Print unique values for inspection
    print(f"{col}: {df[col].unique()}")

# converting necessary columns to numerical type
for col in ['packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count']:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# output into binary
df['class'] = df['class'].map({'ckd': 1, 'notckd': 0})
df['class'] = pd.to_numeric(df['class'], errors='coerce')

"""### Ordinal Encoding"""

from sklearn.preprocessing import OrdinalEncoder

encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
df[cat_cols] = encoder.fit_transform(df[cat_cols])

"""### Separate Class & Features"""

X = df.drop('class', axis=1)
y = df['class']

"""### Iterative Imputer for missing values"""

# Iterative Imputer for missing values
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imputer = IterativeImputer(random_state=42)
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

X_imputed.info()

"""#Model Train Without feature selection

##Random Forest
"""

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import (
    confusion_matrix, roc_auc_score, roc_curve, accuracy_score,
    f1_score, precision_score, recall_score, fbeta_score
)
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

# ==============================
# DATA PREPARATION
# ==============================
# Assuming you already have:
X_eval = X_imputed
y_eval = y
# Make sure y_eval is 1D (Series)
y_eval = np.ravel(y_eval)

# ==============================
# MODEL SETUP
# ==============================
test_size = 0.3
n_splits = 10

rf_model = RandomForestClassifier(random_state=42)

# ==============================
# SPLIT DATA
# ==============================
X_train, X_test, y_train, y_test = train_test_split(
    X_eval, y_eval, test_size=test_size, random_state=42, stratify=y_eval
)

print(f"\n=== Data split with test size {test_size} (train size {1 - test_size}) ===")

# ==============================
# STRATIFIED 10-FOLD CV
# ==============================
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
cv_all_folds = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train[train_idx], y_train[val_idx]

    rf_model.fit(X_tr, y_tr)
    y_val_pred = rf_model.predict(X_val)
    y_val_proba = rf_model.predict_proba(X_val)[:, 1]

    # Safe confusion matrix
    cm = confusion_matrix(y_val, y_val_pred, labels=[0, 1])
    tn, fp, fn, tp = cm.ravel()

    fold_metrics = {
        "Fold": fold,
        "Sensitivity": recall_score(y_val, y_val_pred, zero_division=0),
        "NPV": tn / (tn + fn) if (tn + fn) > 0 else 0,
        "F2": fbeta_score(y_val, y_val_pred, beta=2, zero_division=0),
        "Precision": precision_score(y_val, y_val_pred, zero_division=0),
        "Specificity": tn / (tn + fp) if (tn + fp) > 0 else 0,
        "AUC": roc_auc_score(y_val, y_val_proba) if len(np.unique(y_val)) > 1 else np.nan,
        "F1": f1_score(y_val, y_val_pred, zero_division=0),
        "Accuracy": accuracy_score(y_val, y_val_pred)
    }

    cv_all_folds.append(fold_metrics)
    print(f"Fold {fold:02d}: Accuracy={fold_metrics['Accuracy']:.4f}, AUC={fold_metrics['AUC']:.4f}")

# ==============================
# AVERAGE CV METRICS
# ==============================
avg_cv_metrics = {k: np.nanmean([fold[k] for fold in cv_all_folds if k != "Fold"])
                  for k in cv_all_folds[0] if k != "Fold"}

print("\n=== Average 10-Fold CV Metrics ===")
for m, val in avg_cv_metrics.items():
    print(f"{m:<12}: {val:.4f}")

# ==============================
# FINAL TRAINING & TESTING
# ==============================
start_train = time.time()
rf_model.fit(X_train, y_train)
end_train = time.time()

start_test = time.time()
y_pred = rf_model.predict(X_test)
y_proba = rf_model.predict_proba(X_test)[:, 1]
end_test = time.time()

train_time = end_train - start_train
test_time = end_test - start_test

# ==============================
# TEST METRICS
# ==============================
cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
tn, fp, fn, tp = cm.ravel()

sensitivity = recall_score(y_test, y_pred)
npv = tn / (tn + fn) if (tn + fn) > 0 else 0
f2 = fbeta_score(y_test, y_pred, beta=2)

precision = precision_score(y_test, y_pred)
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
auc = roc_auc_score(y_test, y_proba)
f1 = f1_score(y_test, y_pred)
acc = accuracy_score(y_test, y_pred)

print("\nConfusion Matrix (Test Set):")
print(cm)

print("\n--- Primary Metrics ---")
print(f"Sensitivity/Recall: {sensitivity:.4f}")
print(f"NPV:               {npv:.4f}")
print(f"F2 Score:          {f2:.4f}")

print("\n--- Secondary Metrics ---")
print(f"Precision:         {precision:.4f}")
print(f"Specificity:       {specificity:.4f}")
print(f"AUC-ROC:           {auc:.4f}")
print(f"F1 Score:          {f1:.4f}")
print(f"Accuracy:          {acc:.4f}")

print("\n--- Timing ---")
print(f"Training time: {train_time:.4f} sec")
print(f"Testing time:  {test_time:.4f} sec")

# ==============================
# ROC CURVE
# ==============================
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure()
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.4f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve (Test set)')
plt.legend(loc='lower right')
plt.show()

# ==============================
# METRIC SUMMARY TABLES
# ==============================
df_cv_metrics = pd.DataFrame(cv_all_folds)
print("\n=== Cross-Validation Metrics per Fold ===")
print(df_cv_metrics)

print("\n=== Average Cross-Validation Metrics ===")
print(pd.Series(avg_cv_metrics))

"""### Feature Importance - Random Forest"""

# Get feature importances from the trained Random Forest model
rf_feature_importances = rf_model.feature_importances_

# Create a pandas Series for easier handling and sorting
rf_importance_series = pd.Series(rf_feature_importances, index=X_imputed.columns)

# Sort the features by importance
rf_sorted_importance = rf_importance_series.sort_values(ascending=False)

# Print the sorted feature importances
print("Random Forest Feature Importances:")
print(rf_sorted_importance)

# Optional: Plot the feature importances
plt.figure(figsize=(10, 6))
rf_sorted_importance.plot(kind='bar')
plt.title('Random Forest Feature Importances')
plt.ylabel('Importance')
plt.xlabel('Features')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

"""##XG-Boost"""

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import (
    confusion_matrix, roc_auc_score, roc_curve, accuracy_score,
    f1_score, precision_score, recall_score, fbeta_score
)
from sklearn.model_selection import train_test_split, StratifiedKFold
import xgboost as xgb

# ==============================
# DATA PREPARATION
# ==============================

# X_eval = X_imputed
# y_eval = y
y_eval = np.ravel(y_eval)

# ==============================
# MODEL SETUP (Best Parameters)
# ==============================
xgb_model = xgb.XGBClassifier(
    random_state=42,
    eval_metric='logloss',
    subsample=0.9,
    reg_lambda=3,
    reg_alpha=1,
    n_estimators=200,
    max_depth=5,
    learning_rate=0.01,
    colsample_bytree=0.8
)

# ==============================
# SPLIT DATA
# ==============================
test_size = 0.3
n_splits = 10

X_train, X_test, y_train, y_test = train_test_split(
    X_eval, y_eval, test_size=test_size, random_state=42, stratify=y_eval
)

print(f"\n=== Data split with test size {test_size} (train size {1 - test_size}) ===")

# ==============================
# STRATIFIED 10-FOLD CV
# ==============================
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
cv_all_folds = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train[train_idx], y_train[val_idx]

    xgb_model.fit(X_tr, y_tr)
    y_val_pred = xgb_model.predict(X_val)
    y_val_proba = xgb_model.predict_proba(X_val)[:, 1]

    # Safe confusion matrix
    cm = confusion_matrix(y_val, y_val_pred, labels=[0, 1])
    tn, fp, fn, tp = cm.ravel()

    fold_metrics = {
        "Fold": fold,
        "Sensitivity": recall_score(y_val, y_val_pred, zero_division=0),
        "NPV": tn / (tn + fn) if (tn + fn) > 0 else 0,
        "F2": fbeta_score(y_val, y_val_pred, beta=2, zero_division=0),
        "Precision": precision_score(y_val, y_val_pred, zero_division=0),
        "Specificity": tn / (tn + fp) if (tn + fp) > 0 else 0,
        "AUC": roc_auc_score(y_val, y_val_proba) if len(np.unique(y_val)) > 1 else np.nan,
        "F1": f1_score(y_val, y_val_pred, zero_division=0),
        "Accuracy": accuracy_score(y_val, y_val_pred)
    }

    cv_all_folds.append(fold_metrics)
    print(f"Fold {fold:02d}: Accuracy={fold_metrics['Accuracy']:.4f}, AUC={fold_metrics['AUC']:.4f}")

# ==============================
# AVERAGE CV METRICS
# ==============================
avg_cv_metrics = {k: np.nanmean([fold[k] for fold in cv_all_folds if k != "Fold"])
                  for k in cv_all_folds[0] if k != "Fold"}

print("\n=== Average 10-Fold CV Metrics ===")
for m, val in avg_cv_metrics.items():
    print(f"{m:<12}: {val:.4f}")

# ==============================
# FINAL TRAINING & TESTING
# ==============================
start_train = time.time()
xgb_model.fit(X_train, y_train)
end_train = time.time()

start_test = time.time()
y_pred = xgb_model.predict(X_test)
y_proba = xgb_model.predict_proba(X_test)[:, 1]
end_test = time.time()

train_time = end_train - start_train
test_time = end_test - start_test

# ==============================
# TEST METRICS
# ==============================
cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
tn, fp, fn, tp = cm.ravel()

sensitivity = recall_score(y_test, y_pred)
npv = tn / (tn + fn) if (tn + fn) > 0 else 0
f2 = fbeta_score(y_test, y_pred, beta=2)

precision = precision_score(y_test, y_pred)
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
auc = roc_auc_score(y_test, y_proba)
f1 = f1_score(y_test, y_pred)
acc = accuracy_score(y_test, y_pred)

print("\nConfusion Matrix (Test Set):")
print(cm)

print("\n--- Primary Metrics ---")
print(f"Sensitivity/Recall: {sensitivity:.4f}")
print(f"NPV:               {npv:.4f}")
print(f"F2 Score:          {f2:.4f}")

print("\n--- Secondary Metrics ---")
print(f"Precision:         {precision:.4f}")
print(f"Specificity:       {specificity:.4f}")
print(f"AUC-ROC:           {auc:.4f}")
print(f"F1 Score:          {f1:.4f}")
print(f"Accuracy:          {acc:.4f}")

print("\n--- Timing ---")
print(f"Training time: {train_time:.4f} sec")
print(f"Testing time:  {test_time:.4f} sec")

# ==============================
# ROC CURVE
# ==============================
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure()
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.4f})', color='blue')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve (XGBoost, Test set)')
plt.legend(loc='lower right')
plt.show()

# ==============================
# METRIC SUMMARY TABLES
# ==============================
df_cv_metrics = pd.DataFrame(cv_all_folds)
print("\n=== Cross-Validation Metrics per Fold ===")
print(df_cv_metrics)

print("\n=== Average Cross-Validation Metrics ===")
print(pd.Series(avg_cv_metrics))

"""### Feature Importance - XGBoost"""

# Get feature importances from the trained XGBoost model
xgb_feature_importances = xgb_model.feature_importances_

# Create a pandas Series for easier handling and sorting
xgb_importance_series = pd.Series(xgb_feature_importances, index=X_imputed.columns)

# Sort the features by importance
xgb_sorted_importance = xgb_importance_series.sort_values(ascending=False)

# Print the sorted feature importances
print("XGBoost Feature Importances:")
print(xgb_sorted_importance)

# Optional: Plot the feature importances using xgb.plot_importance
# Note: xgb.plot_importance uses the internal feature names (f0, f1, etc.) by default
# You might need to map them back to original column names for a clearer plot.
# Alternatively, you can use the sorted importance series with pandas plotting as done for RF.
xgb.plot_importance(xgb_model)
plt.title('XGBoost Feature Importance (Internal Feature Names)')
plt.show()

# Plot using pandas with actual feature names
plt.figure(figsize=(10, 6))
xgb_sorted_importance.plot(kind='bar')
plt.title('XGBoost Feature Importances')
plt.ylabel('Importance')
plt.xlabel('Features')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

"""### Overlapping High-Ranking Features"""

# Define a threshold for 'high-ranking' (e.g., top 10 features)
top_n = 10

# Get the top features from each model
rf_top_features = rf_sorted_importance.head(top_n).index.tolist()
xgb_top_features = xgb_sorted_importance.head(top_n).index.tolist()

print(f"Top {top_n} features from Random Forest:")
print(rf_top_features)

print(f"\nTop {top_n} features from XGBoost:")
print(xgb_top_features)

# Find the overlapping features
overlapping_features = list(set(rf_top_features) & set(xgb_top_features))

print(f"\nOverlapping top  features:")
print(overlapping_features)

# Find the total features
all_dominant_features = list(set(rf_top_features) | set(xgb_top_features))

print(f"\nAll top features:")
print(all_dominant_features)

# Identify and note the dominant clinical markers within the overlapping features
clinical_markers = ["serum_creatinine", "haemoglobin", "red_blood_cell_count", "albumin", "blood_glucose_random", "blood_pressure", "blood_urea", "sodium", "potassium"] # Add relevant clinical markers
dominant_markers_in_overlap = [feature for feature in all_dominant_features if feature in clinical_markers]

print("\nDominant clinical markers within overlapping top features:")
print(dominant_markers_in_overlap)

"""#PSO train

###Prepare Dataset for PSO
"""

# Import libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split

# 30% test split, keep class balance
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed,
    y,
    test_size=0.3,
    stratify=y,
    random_state=42
)

print(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")

X_pso = X_train[all_dominant_features].copy()
y_pso = y_train.copy()

print(f"PSO dataset shape: {X_pso.shape}")

"""###Define the Objective Function"""

# Function to evaluate feature subset
from sklearn.model_selection import cross_validate

def pso_objective(particle):
    n_particles = particle.shape[0]
    fitness = np.zeros(n_particles)
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')

    for i in range(n_particles):
        mask = particle[i].astype(bool)
        if np.sum(mask) == 0:
            fitness[i] = 1.0
            continue

        X_subset = X_pso.iloc[:, mask]
        metrics = cross_validate(rf, X_subset, y_pso, cv=cv,
                                 scoring=['recall', 'precision'])

        recall_mean = metrics['test_recall'].mean()
        precision_mean = metrics['test_precision'].mean()

        # Weighted penalty (favor recall)
        fitness[i] = (1 - (0.7 * recall_mean + 0.3 * precision_mean)) \
                     + 0.01 * (np.sum(mask) / X_pso.shape[1])

    return fitness

"""###Configure Binary PSO"""

try:
    from pyswarms.discrete import BinaryPSO
except ImportError:
    !pip install pyswarms
    from pyswarms.discrete import BinaryPSO

# Define PSO parameters
n_particles = 30
dimensions = X_pso.shape[1]  # number of overlapping features
iterations = 50

# Options must include k and p
options = {
    'c1': 1.5,   # cognitive parameter
    'c2': 1.5,   # social parameter
    'w': 0.7,    # inertia weight
    'k': 5,      # number of neighbors to use
    'p': 2       # Minkowski p-norm (2 = Euclidean)
}

# Initialize Binary PSO optimizer
optimizer = BinaryPSO(n_particles=n_particles, dimensions=dimensions, options=options)

"""###Run the PSO Optimization"""

from collections import Counter
feature_counter = Counter()
for run in range(5):
    best_cost, best_pos = optimizer.optimize(pso_objective, iters=iterations)
    subset = [f for f, b in zip(all_dominant_features, best_pos) if b == 1]
    feature_counter.update(subset)

print("Feature selection frequency:")
print(feature_counter.most_common())

"""###Extract Minimal Feature Subset"""

# Map the binary mask to feature names
selected_features = [feature for feature, bit in zip(all_dominant_features, best_pos) if bit == 1]

print("Minimal Feature Subset (MFS) selected by PSO:")
print(selected_features)

import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
import pyswarms as ps

# Use only the overlapping subset for PSO exploration
X_sub = X_imputed[overlapping_features].values
y_sub = y.values
n_features = X_sub.shape[1]

base_model = RandomForestClassifier(random_state=42)
alpha = 0.005  # small penalty to favor fewer features

def pso_validation_fitness(particles):
    scores = []
    for particle in particles:
        mask = particle > 0.5
        if np.sum(mask) == 0:
            scores.append(1.0)
            continue

        X_sel = X_sub[:, mask]
        acc = cross_val_score(base_model, X_sel, y_sub, cv=5, scoring='accuracy').mean()
        # penalize feature count slightly
        fitness = (1 - acc) + alpha * (np.sum(mask) / n_features)
        scores.append(fitness)
    return np.array(scores)

# Initialize PSO (focused local search)
options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}
optimizer = ps.single.GlobalBestPSO(n_particles=20, dimensions=n_features, options=options)

cost, pos = optimizer.optimize(pso_validation_fitness, iters=20)

# Extract PSO-selected features
selected_mask = pos > 0.5
pso_selected_features = np.array(overlapping_features)[selected_mask]

print("\n=== PSO Validation Complete ===")
print(f"Best Fitness Score: {cost:.5f}")
print(f"Selected Features ({selected_mask.sum()} of {n_features}):")
print(list(pso_selected_features))

# ============================================
# Final Cross-Validation Evaluation Script
# ============================================

# Imports
from sklearn.model_selection import cross_validate, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import make_scorer, recall_score, precision_score, f1_score, balanced_accuracy_score
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

# ============================================
# Dataset
# ============================================

# Ensure 'pso_selected_features' or 'selected_features' is defined correctly
X_final = X_test[pso_selected_features]   # or X_test[selected_features]
y_final = y_test

# ============================================
# Model Initialization with Tuned Defaults
# ============================================

rf_final = RandomForestClassifier(
    n_estimators=300,
    max_depth=8,
    min_samples_leaf=3,
    max_features='sqrt',
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

xgb_final = xgb.XGBClassifier(
    subsample=0.9,
    reg_lambda=3,
    reg_alpha=1,
    n_estimators=200,
    max_depth=5,
    learning_rate=0.01,
    colsample_bytree=0.8,
    eval_metric='logloss',
    random_state=42,
    n_jobs=-1
)


# ============================================
# Cross-Validation Setup
# ============================================

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

scorers = {
    'accuracy': 'accuracy',
    'roc_auc': 'roc_auc',
    'recall': make_scorer(recall_score),
    'precision': make_scorer(precision_score),
    'f1': make_scorer(f1_score),
    'balanced_acc': make_scorer(balanced_accuracy_score)
}

# ============================================
# Evaluation Loop
# ============================================

for model, name in [(rf_final, "Random Forest"), (xgb_final, "XGBoost")]:
    print(f"\n{name} Performance on PSO-selected features ({len(pso_selected_features)} features):")

    scores = cross_validate(model, X_final, y_final, cv=cv, scoring=scorers, n_jobs=-1)

    print(f"Accuracy:            {scores['test_accuracy'].mean():.4f}")
    print(f"ROC-AUC:             {scores['test_roc_auc'].mean():.4f}")
    print(f"Recall (Sensitivity): {scores['test_recall'].mean():.4f}")
    print(f"Precision:           {scores['test_precision'].mean():.4f}")
    print(f"F1 Score:            {scores['test_f1'].mean():.4f}")
    print(f"Balanced Accuracy:   {scores['test_balanced_acc'].mean():.4f}")

# ============================================
# (Optional) Hyperparameter Optimization for XGBoost
# ============================================

# Uncomment this block if you want to perform RandomizedSearchCV


param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.6, 0.8, 0.9],
    'colsample_bytree': [0.6, 0.8, 0.9],
    'reg_alpha': [0.01, 0.1, 1],
    'reg_lambda': [1, 3, 5]
}

xgb_search = RandomizedSearchCV(
    xgb_final,
    param_distributions=param_dist,
    n_iter=20,
    scoring='roc_auc',
    cv=cv,
    n_jobs=-1,
    random_state=42,
    verbose=1
)

xgb_search.fit(X_final, y_final)
print("\nBest XGBoost Parameters:")
print(xgb_search.best_params_)

import shap

# Use the final Random Forest model
best_rf_model = rf_final

# Fit the best model on the training data with the selected features
X_train_final = X_train[pso_selected_features]
best_rf_model.fit(X_train_final, y_train)

# Create a SHAP explainer
explainer = shap.TreeExplainer(best_rf_model)

# Calculate SHAP values for the test set
X_test_final = X_test[pso_selected_features]
shap_values = explainer.shap_values(X_test_final)

# Visualize the SHAP values

# Summary plot
print("\nSHAP Summary Plot:")
# Corrected indexing for shap_values (assuming (n_samples, n_features, n_classes) structure)
shap.summary_plot(shap_values[:, :, 1], X_test_final)

# Dependence plot (example for the most important feature)
# Calculate mean absolute SHAP values for the positive class to find the most important feature
mean_abs_shap_values = np.abs(shap_values[:, :, 1]).mean(0) # Corrected
most_important_feature_index = np.argmax(mean_abs_shap_values)
most_important_feature = pso_selected_features[most_important_feature_index]

print(f"\nSHAP Dependence Plot for '{most_important_feature}':")
shap.dependence_plot(most_important_feature, shap_values[:, :, 1], X_test_final) # Corrected

# Force plot (example for the first prediction)
print("\nSHAP Force Plot for the first test instance:")
shap.initjs()
# Corrected for the first sample, all features, for class 1
shap.force_plot(explainer.expected_value[1], shap_values[0, :, 1], X_test_final.iloc[0,:])